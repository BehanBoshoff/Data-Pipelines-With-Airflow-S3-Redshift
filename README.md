# Data-Pipelines-With-Airflow-S3-Redshift
An Apache Airflow project containing dynamic automated data pipelines with custom operators.
The custom operators form reusable tasks that perform functions such as staging the data from AWS S3 into Redshift, ETL processes to fill the data warehouse by forming a fact & dimension star-schema in AWS Redshift, and running data quality checks to verify data integrity.
